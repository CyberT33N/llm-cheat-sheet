# LLM cheat sheet

<br><br>

## Guides
- https://www.youtube.com/watch?v=zjkBMFhNj_g

<br><br>

# Structure
- E.g. llama-2-70b
  - Contains 2 files

    - **parameters**: Weights of the neural network 
      - 140GB big because 70b of the LLM is stored in 2 bytes (float16)

	- **run**: Lines of c code to run the LLM
	  - Can be in any programming language


<br><br>

# Training
- You can understand that you will collect informations as text from websites in a big amount like e.g. 10TB
  -> Train with 6000 GPUS for 12 days (2$ million dollar)
    -> Compress the information to parameters.zip (140GB) 
      - Not really a .zip doe but easier to understand


<br><br>

# Neural network
- aka next work prediction neural network
  - E.g. you give the sentence `cat sat on a` as sequence then it will predicts the next word (mat 97%).
	- There is mathematically relationship between prediction and compression. So this is why we called it compression in the section training. Because if you can predict the nord word you can compress the dataset.

<br><br>

## LLM dreams
- So when a neural network will predict the next word you can think of web page dreaming because the neural network was trained with web pages. E.g. if you have an amazon product page then ISBN: 2324342424242 would look this this.
  - So if the ISBN number was generated by the LLM then it will not exists but it will know the length and format because it was trained with data like this.
  - The same dreaming logic goes for code or blog articles
    - **This is the reason why you often get falsy informations when you ask questions because the LLM just predicts what it thinks will be right for the next sequence**
